"""
Scanning and indexing logic for Movie Searcher.
Handles directory scanning, movie indexing, and progress tracking.
"""
import os
import json
import re
import hashlib
import logging
import threading
from pathlib import Path
from datetime import datetime
from typing import Optional, Callable
from sqlalchemy.orm import Session

# Database imports
from database import SessionLocal, Movie, Screenshot, Image, IndexedPath, Config

# Video processing imports
from video_processing import (
    shutdown_flag, frame_extraction_queue,
    get_video_length as get_video_length_vp,
    extract_screenshots as extract_screenshots_core,
    extract_movie_screenshot as extract_movie_screenshot_core,
    find_ffmpeg as find_ffmpeg_core,
    process_frame_queue as process_frame_queue_core
)

logger = logging.getLogger(__name__)

# File extensions
VIDEO_EXTENSIONS = {'.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mpg', '.mpeg', '.3gp'}
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}

# Scan progress tracking (in-memory)
scan_progress = {
    "is_scanning": False,
    "current": 0,
    "total": 0,
    "current_file": "",
    "status": "idle",
    "logs": [],  # List of log entries: {"timestamp": str, "level": str, "message": str}
    "frame_queue_size": 0,
    "frames_processed": 0,
    "frames_total": 0
}

# Callbacks for external functions (to avoid circular imports)
_load_config_callback: Optional[Callable] = None
_get_movies_folder_callback: Optional[Callable] = None

def set_callbacks(load_config_func: Callable, get_movies_folder_func: Callable):
    """Set callbacks for functions from main module to avoid circular imports"""
    global _load_config_callback, _get_movies_folder_callback
    _load_config_callback = load_config_func
    _get_movies_folder_callback = get_movies_folder_func

def load_config():
    """Load configuration using callback"""
    if _load_config_callback:
        return _load_config_callback()
    return {}

def get_movies_folder():
    """Get movies folder using callback"""
    if _get_movies_folder_callback:
        return _get_movies_folder_callback()
    return None

def add_scan_log(level: str, message: str):
    """Add a log entry to scan progress"""
    global scan_progress
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_entry = {
        "timestamp": timestamp,
        "level": level,  # "info", "success", "warning", "error"
        "message": message
    }
    scan_progress["logs"].append(log_entry)
    # Keep only last 1000 log entries to prevent memory issues
    if len(scan_progress["logs"]) > 1000:
        scan_progress["logs"] = scan_progress["logs"][-1000:]

def is_sample_file(file_path):
    """Check if a file should be excluded (contains 'sample' in name, case-insensitive)"""
    if isinstance(file_path, Path):
        name = file_path.stem.lower()
    else:
        name = Path(file_path).stem.lower()
    return 'sample' in name

def get_file_hash(file_path):
    """Generate hash for file to detect changes"""
    stat = os.stat(file_path)
    return hashlib.md5(f"{file_path}:{stat.st_mtime}:{stat.st_size}".encode()).hexdigest()

def find_images_in_folder(video_path):
    """
    Find image files in the same folder as the video.
    
    These are "images" - media files that came with the movie (posters, covers, thumbnails, etc.).
    These are existing files on disk, not generated by us.
    
    Returns a list of image file paths found in the video's folder.
    """
    video_path_obj = Path(video_path)
    video_dir = video_path_obj.parent
    base_name = video_path_obj.stem
    
    images = []
    for ext in IMAGE_EXTENSIONS:
        # Check for exact match
        img_path = video_dir / f"{base_name}{ext}"
        if img_path.exists() and "www.YTS.AM" not in img_path.name:
            images.append(str(img_path))
        
        # Check for common patterns (poster, cover, etc.)
        for pattern in [f"{base_name}_poster{ext}", f"{base_name}_cover{ext}", f"{base_name}_thumb{ext}",
                        f"poster{ext}", f"cover{ext}", f"folder{ext}", f"thumb{ext}"]:
            img_path = video_dir / pattern
            if img_path.exists() and str(img_path) not in images and "www.YTS.AM" not in img_path.name:
                images.append(str(img_path))
    
    # Also check for any images in the folder (limit to first 10)
    for img_file in video_dir.iterdir():
        if img_file.suffix.lower() in IMAGE_EXTENSIONS and str(img_file) not in images and "www.YTS.AM" not in img_file.name:
            images.append(str(img_file))
            if len(images) >= 10:
                break
    
    return images[:10]  # Limit to 10 images

def filter_yts_images(image_paths):
    """Filter out images with 'www.YTS.AM' in filename"""
    if not image_paths:
        return []
    filtered = []
    for img_path in image_paths:
        # Check if filename contains www.YTS.AM
        img_name = Path(img_path).name
        if "www.YTS.AM" not in img_name:
            filtered.append(img_path)
    return filtered

def extract_year_from_name(name):
    """Extract year from movie name (1900-2035)"""
    # Look for 4-digit years in the range 1900-2035
    year_pattern = r'\b(19\d{2}|20[0-2]\d|203[0-5])\b'
    matches = re.findall(year_pattern, name)
    if matches:
        # Return the first valid year found
        year = int(matches[0])
        if 1900 <= year <= 2035:
            return year
    return None

def load_cleaning_patterns():
    """Load approved cleaning patterns from database"""
    db = SessionLocal()
    try:
        config_row = db.query(Config).filter(Config.key == 'cleaning_patterns').first()
        if config_row:
            try:
                data = json.loads(config_row.value)
                return {
                    'exact_strings': set(data.get('exact_strings', [])),
                    'bracket_patterns': data.get('bracket_patterns', []),
                    'parentheses_patterns': data.get('parentheses_patterns', []),
                    'year_patterns': data.get('year_patterns', True),  # Default to True
                }
            except Exception as e:
                logger.error(f"Error parsing cleaning patterns from database: {e}")
    except Exception as e:
        logger.error(f"Error loading cleaning patterns: {e}")
    finally:
        db.close()
    
    # Return defaults if not found
    return {
        'exact_strings': set(),
        'bracket_patterns': [],
        'parentheses_patterns': [],
        'year_patterns': True,
    }

def clean_movie_name(name, patterns=None):
    """Clean movie name using approved patterns and extract year"""
    if patterns is None:
        patterns = load_cleaning_patterns()
    
    original_name = name
    year = None
    
    # Extract year first if enabled
    if patterns.get('year_patterns', True):
        year = extract_year_from_name(name)
        # Remove year from name
        if year:
            name = re.sub(rf'\b{year}\b', '', name)
    
    # Remove exact strings
    for exact_str in patterns.get('exact_strings', set()):
        name = name.replace(exact_str, ' ')
    
    # Remove bracket patterns [anything]
    for pattern in patterns.get('bracket_patterns', []):
        if pattern == '[anything]':
            name = re.sub(r'\[.*?\]', '', name)
        else:
            name = name.replace(pattern, ' ')
    
    # Remove parentheses patterns (anything)
    for pattern in patterns.get('parentheses_patterns', []):
        if pattern == '(anything)':
            # Remove parentheses content, but be smart about it
            # Don't remove if it's just a year or looks like part of title
            name = re.sub(r'\([^)]*\)', '', name)
        else:
            name = name.replace(pattern, ' ')
    
    # Clean up multiple spaces and trim
    name = re.sub(r'\s+', ' ', name).strip()
    
    # If name becomes empty, use original
    if not name:
        name = original_name
    
    return name, year

def get_video_length(file_path):
    """Extract video length using video_processing module"""
    return get_video_length_vp(file_path)

def extract_screenshots(video_path, num_screenshots=5):
    """
    Extract screenshots from video using ffmpeg.
    
    These are "screenshots" - frames extracted from the video file itself.
    These are generated by us during scanning, not pre-existing files.
    
    Returns a list of screenshot file paths.
    """
    return extract_screenshots_core(video_path, num_screenshots, load_config, find_ffmpeg_core)

def extract_movie_screenshot(video_path, timestamp_seconds=150, async_mode=True):
    """Extract a single screenshot from video - can be synchronous or queued for async processing"""
    if async_mode:
        # Use video_processing module's extract_movie_screenshot which handles async queuing
        return extract_movie_screenshot_core(
            video_path, timestamp_seconds, async_mode,
            load_config, find_ffmpeg_core, scan_progress, add_scan_log
        )
    else:
        # Synchronous mode - import here to avoid circular dependency
        from video_processing import extract_movie_screenshot_sync
        return extract_movie_screenshot_sync(video_path, timestamp_seconds, lambda: find_ffmpeg_core(load_config))

def process_frame_queue(max_workers=3):
    """Process queued frame extractions in background thread pool"""
    process_frame_queue_core(max_workers, scan_progress, add_scan_log)

def index_movie(file_path, db: Session = None):
    """Index a single movie file"""
    # Normalize the path to ensure consistent storage
    # file_path can be either a Path object or a string
    if isinstance(file_path, Path):
        path_obj = file_path
    else:
        path_obj = Path(file_path)
    
    # Use resolve() to get absolute normalized path
    try:
        normalized_path_obj = path_obj.resolve()
    except (OSError, RuntimeError):
        # If resolve fails, use absolute()
        normalized_path_obj = path_obj.absolute()
    
    # Convert to string - Path objects on Windows already use backslashes
    normalized_path = str(normalized_path_obj)
    
    file_hash = get_file_hash(normalized_path)
    
    # Use provided session or create new one
    should_close = False
    if db is None:
        db = SessionLocal()
        should_close = True
    
    try:
        # Check if already indexed and unchanged
        existing = db.query(Movie).filter(Movie.path == normalized_path).first()
        file_unchanged = existing and existing.hash == file_hash
        
        # Check if screenshot exists for this movie
        existing_screenshot = None
        if existing:
            existing_screenshot = db.query(Screenshot).filter(Screenshot.movie_id == existing.id).first()
        has_screenshot = existing_screenshot and os.path.exists(existing_screenshot.shot_path) if existing_screenshot else False
        
        # If file unchanged and screenshot exists, no update needed
        if file_unchanged and has_screenshot:
            return False  # No update needed
        
        add_scan_log("info", f"  Getting file metadata...")
        
        stat = os.stat(normalized_path)
        created = datetime.fromtimestamp(stat.st_ctime)
        size = stat.st_size
        
        # Try to get video length
        length = get_video_length(normalized_path)
        
        # Find images in folder
        # "images" = media files that came with the movie (posters, covers, etc.)
        add_scan_log("info", f"  Searching for images in folder...")
        images = find_images_in_folder(normalized_path)
        if images:
            add_scan_log("success", f"  Found {len(images)} image(s)")
        
        # Check for existing screenshots in database
        existing_screenshots_list = []
        if existing:
            existing_screenshots_list = [s.shot_path for s in db.query(Screenshot).filter(Screenshot.movie_id == existing.id).all()]
            # Filter to only those that still exist on disk
            existing_screenshots_list = [s for s in existing_screenshots_list if os.path.exists(s)]
        
        # Extract screenshots if we don't have any yet
        # Extract if: no images found OR (images found but no existing screenshots)
        screenshots = []
        if len(existing_screenshots_list) > 0:
            screenshots = existing_screenshots_list
            add_scan_log("info", f"  Using {len(screenshots)} existing screenshot(s)")
        elif len(images) == 0 or not existing or len(existing_screenshots_list) == 0:
            add_scan_log("info", f"  Extracting screenshots...")
            screenshots = extract_screenshots(normalized_path, num_screenshots=5)
            if screenshots:
                add_scan_log("success", f"  Extracted {len(screenshots)} screenshot(s)")
        
        # Extract movie screenshot (at 2-3 minutes, default 2.5 minutes = 150 seconds)
        add_scan_log("info", f"  Checking screenshot...")
        if existing_screenshot:
            # Check if the screenshot file still exists
            if os.path.exists(existing_screenshot.shot_path):
                add_scan_log("info", f"  Screenshot already exists")
            else:
                # Screenshot file was deleted, remove from DB and queue for re-extraction
                add_scan_log("warning", f"  Screenshot file missing, queuing re-extraction...")
                db.delete(existing_screenshot)
                extract_movie_screenshot(normalized_path, timestamp_seconds=150, async_mode=True)
        else:
            # No screenshot exists, queue for extraction (even if file unchanged)
            add_scan_log("info", f"  No screenshot found, queuing extraction...")
            extract_movie_screenshot(normalized_path, timestamp_seconds=150, async_mode=True)
        
        # Filter out YTS images before storing in database (defense in depth)
        images = filter_yts_images(images)
        
        # Clean movie name and extract year
        raw_name = normalized_path_obj.stem
        cleaned_name, year = clean_movie_name(raw_name)
        
        # Create or update movie record
        if existing:
            # Update existing movie
            existing.name = cleaned_name
            existing.year = year
            existing.length = length
            existing.size = size
            existing.hash = file_hash
            existing.updated = datetime.now()
            movie = existing
        else:
            # Create new movie
            movie = Movie(
                path=normalized_path,
                name=cleaned_name,
                year=year,
                length=length,
                created=created,
                size=size,
                hash=file_hash
            )
            db.add(movie)
            db.flush()  # Flush to get movie.id
        
        # Store images in Image table
        if images:
            # Delete existing images for this movie
            db.query(Image).filter(Image.movie_id == movie.id).delete()
            # Add new images
            for img_path in images:
                image = Image(movie_id=movie.id, image_path=img_path)
                db.add(image)
        
        # Store screenshots in Screenshot table (from extract_screenshots function)
        if screenshots:
            # Get existing screenshot paths to avoid duplicates
            existing_shot_paths = {s.shot_path for s in db.query(Screenshot).filter(Screenshot.movie_id == movie.id).all()}
            # Add new screenshots (avoid duplicates)
            for shot_path in screenshots:
                if shot_path not in existing_shot_paths:
                    screenshot = Screenshot(movie_id=movie.id, shot_path=shot_path)
                    db.add(screenshot)
        
        db.commit()
        return True
    finally:
        if should_close:
            db.close()

def scan_directory(root_path, state=None, progress_callback=None):
    """Scan directory for video files with optional progress callback
    Each movie commits individually - no transaction wrapping the scan.
    If any movie fails, the entire scan stops immediately.
    """
    root = Path(root_path)
    if not root.exists():
        add_scan_log("error", f"Path does not exist: {root_path}")
        raise ValueError(f"Path does not exist: {root_path}")
    
    add_scan_log("info", f"Starting scan of: {root_path}")
    db = SessionLocal()
    try:
        # First pass: count total files
        global scan_progress
        scan_progress["status"] = "counting"
        scan_progress["current_file"] = "Counting files..."
        add_scan_log("info", "Counting video files...")
        
        total_files = 0
        for ext in VIDEO_EXTENSIONS:
            files = [f for f in root.rglob(f"*{ext}") if not is_sample_file(f)]
            count = len(files)
            total_files += count
            if count > 0:
                add_scan_log("info", f"Found {count} {ext} files")
        
        scan_progress["total"] = total_files
        scan_progress["current"] = 0
        scan_progress["status"] = "scanning"
        add_scan_log("success", f"Total files to process: {total_files}")
        
        indexed = 0
        updated = 0
        
        # Second pass: actually scan
        # Each movie commits individually - no transaction wrapping the scan
        # If any movie fails, the entire scan stops immediately
        add_scan_log("info", "Starting file processing...")
        for ext in VIDEO_EXTENSIONS:
            if shutdown_flag.is_set():
                add_scan_log("warning", "Scan interrupted by shutdown")
                break
            for file_path in root.rglob(f"*{ext}"):
                if shutdown_flag.is_set():
                    add_scan_log("warning", "Scan interrupted by shutdown")
                    break
                
                # Skip sample files
                if is_sample_file(file_path):
                    add_scan_log("info", f"Skipping sample file: {file_path.name}")
                    continue
                
                # Process each movie - if it fails, stop the entire scan immediately
                # Each movie commits individually, no transaction wrapping the whole scan
                scan_progress["current"] = indexed + 1
                scan_progress["current_file"] = file_path.name
                
                add_scan_log("info", f"[{indexed + 1}/{total_files}] Processing: {file_path.name}")
                
                if index_movie(file_path, db):
                    updated += 1
                    add_scan_log("success", f"Indexed: {file_path.name}")
                else:
                    add_scan_log("info", f"Skipped (unchanged): {file_path.name}")
                indexed += 1
                
                if progress_callback:
                    progress_callback(indexed, total_files, file_path.name)
        
        # Mark path as indexed
        db.merge(IndexedPath(path=str(root_path)))
        db.commit()
        
        add_scan_log("success", f"Scan complete: {indexed} files processed, {updated} updated")
        
        return {"indexed": indexed, "updated": updated}
    finally:
        db.close()

def run_scan_async(root_path: str):
    """Run scan in background thread"""
    global scan_progress, frame_extraction_queue
    try:
        if shutdown_flag.is_set():
            return
        scan_progress["is_scanning"] = True
        scan_progress["current"] = 0
        scan_progress["total"] = 0
        scan_progress["current_file"] = ""
        scan_progress["status"] = "starting"
        scan_progress["logs"] = []  # Clear previous logs
        scan_progress["frames_processed"] = 0
        scan_progress["frames_total"] = 0
        
        # Clear frame queue
        while not frame_extraction_queue.empty():
            try:
                frame_extraction_queue.get_nowait()
            except:
                break
        
        add_scan_log("info", "=" * 60)
        add_scan_log("info", "Starting movie scan")
        add_scan_log("info", f"Root path: {root_path}")
        add_scan_log("info", "=" * 60)
        
        # Start frame extraction processing in parallel (if not already running)
        process_frame_queue(max_workers=3)
        
        result = scan_directory(root_path, progress_callback=None)
        
        add_scan_log("info", "=" * 60)
        add_scan_log("success", f"Scan completed successfully!")
        add_scan_log("info", f"  Files processed: {result['indexed']}")
        add_scan_log("info", f"  Files updated: {result['updated']}")
        queue_size = frame_extraction_queue.qsize()
        if queue_size > 0:
            add_scan_log("info", f"  Frames queued: {queue_size} (processing in background)")
        add_scan_log("info", "=" * 60)
        
        scan_progress["status"] = "complete"
        scan_progress["is_scanning"] = False
        logger.info(f"Scan complete: {result}")
    except Exception as e:
        # If any movie fails, stop the entire scan immediately
        # Log the error and mark scan as failed - don't continue processing
        error_msg = str(e)
        add_scan_log("error", f"Scan failed: {error_msg}")
        scan_progress["status"] = "error"
        scan_progress["is_scanning"] = False
        logger.error(f"Scan failed: {e}", exc_info=True)
        # Don't re-raise in background thread - just stop and report error

