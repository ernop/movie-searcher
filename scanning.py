"""
Scanning and indexing logic for Movie Searcher.
Handles directory scanning, movie indexing, and progress tracking.
"""
import os
import json
import re
import hashlib
import logging
import threading
from pathlib import Path
from datetime import datetime
from typing import Optional, Callable
from sqlalchemy.orm import Session
from sqlalchemy.dialects.sqlite import insert as sqlite_insert
from sqlalchemy.sql import func

# Database imports
from database import SessionLocal, Movie, Screenshot, Image, IndexedPath, Config

# Video processing imports
from video_processing import (
    shutdown_flag, frame_extraction_queue,
    get_video_length as get_video_length_vp,
    extract_screenshots as extract_screenshots_core,
    extract_movie_screenshot as extract_movie_screenshot_core,
    find_ffmpeg as find_ffmpeg_core,
    process_frame_queue as process_frame_queue_core
)

logger = logging.getLogger(__name__)

# File extensions
VIDEO_EXTENSIONS = {'.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mpg', '.mpeg', '.3gp'}
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}

# Scan progress tracking (in-memory)
scan_progress = {
    "is_scanning": False,
    "current": 0,
    "total": 0,
    "current_file": "",
    "status": "idle",
    "logs": [],  # List of log entries: {"timestamp": str, "level": str, "message": str}
    "frame_queue_size": 0,
    "frames_processed": 0,
    "frames_total": 0
}

# Callbacks for external functions (to avoid circular imports)
_load_config_callback: Optional[Callable] = None
_get_movies_folder_callback: Optional[Callable] = None

def set_callbacks(load_config_func: Callable, get_movies_folder_func: Callable):
    """Set callbacks for functions from main module to avoid circular imports"""
    global _load_config_callback, _get_movies_folder_callback
    _load_config_callback = load_config_func
    _get_movies_folder_callback = get_movies_folder_func

def load_config():
    """Load configuration using callback"""
    if _load_config_callback:
        return _load_config_callback()
    return {}

def get_movies_folder():
    """Get movies folder using callback"""
    if _get_movies_folder_callback:
        return _get_movies_folder_callback()
    return None

def add_scan_log(level: str, message: str):
    """Add a log entry to scan progress"""
    global scan_progress
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_entry = {
        "timestamp": timestamp,
        "level": level,  # "info", "success", "warning", "error"
        "message": message
    }
    scan_progress["logs"].append(log_entry)
    # Keep only last 1000 log entries to prevent memory issues
    if len(scan_progress["logs"]) > 1000:
        scan_progress["logs"] = scan_progress["logs"][-1000:]

def is_sample_file(file_path):
    """Check if a file should be excluded (contains 'sample' in name, case-insensitive)"""
    if isinstance(file_path, Path):
        name = file_path.stem.lower()
    else:
        name = Path(file_path).stem.lower()
    return 'sample' in name

def get_file_hash(file_path):
    """Generate hash for file to detect changes"""
    stat = os.stat(file_path)
    return hashlib.md5(f"{file_path}:{stat.st_mtime}:{stat.st_size}".encode()).hexdigest()

def find_images_in_folder(video_path):
    """
    Find image files in the same folder as the video.
    
    These are "images" - media files that came with the movie (posters, covers, thumbnails, etc.).
    These are existing files on disk, not generated by us.
    
    Returns a list of image file paths found in the video's folder.
    """
    video_path_obj = Path(video_path)
    video_dir = video_path_obj.parent
    base_name = video_path_obj.stem
    
    images = []
    for ext in IMAGE_EXTENSIONS:
        # Check for exact match
        img_path = video_dir / f"{base_name}{ext}"
        if img_path.exists() and "www.YTS.AM" not in img_path.name:
            images.append(str(img_path))
        
        # Check for common patterns (poster, cover, etc.)
        for pattern in [f"{base_name}_poster{ext}", f"{base_name}_cover{ext}", f"{base_name}_thumb{ext}",
                        f"poster{ext}", f"cover{ext}", f"folder{ext}", f"thumb{ext}"]:
            img_path = video_dir / pattern
            if img_path.exists() and str(img_path) not in images and "www.YTS.AM" not in img_path.name:
                images.append(str(img_path))
    
    # Also check for any images in the folder (limit to first 10)
    for img_file in video_dir.iterdir():
        if img_file.suffix.lower() in IMAGE_EXTENSIONS and str(img_file) not in images and "www.YTS.AM" not in img_file.name:
            images.append(str(img_file))
            if len(images) >= 10:
                break
    
    return images[:10]  # Limit to 10 images

def filter_yts_images(image_paths):
    """Filter out images with 'www.YTS.AM' in filename"""
    if not image_paths:
        return []
    filtered = []
    for img_path in image_paths:
        # Check if filename contains www.YTS.AM
        img_name = Path(img_path).name
        if "www.yts" in img_name.lower():
            continue
        if "www.yify" in img_name.lower():
            continue
        if "torrents" in img_name.lower():
            continue
        filtered.append(img_path)
    return filtered

def extract_year_from_name(name):
    """Extract year from movie name (1900-2035)"""
    # Look for 4-digit years in the range 1900-2035
    year_pattern = r'\b(19\d{2}|20[0-2]\d|203[0-5])\b'
    matches = re.findall(year_pattern, name)
    if matches:
        # Return the first valid year found
        year = int(matches[0])
        if 1900 <= year <= 2035:
            return year
    return None

def load_cleaning_patterns():
    """Load approved cleaning patterns from database"""
    db = SessionLocal()
    try:
        config_row = db.query(Config).filter(Config.key == 'cleaning_patterns').first()
        if config_row:
            try:
                data = json.loads(config_row.value)
                return {
                    'exact_strings': set(data.get('exact_strings', [])),
                    'bracket_patterns': data.get('bracket_patterns', []),
                    'parentheses_patterns': data.get('parentheses_patterns', []),
                    'year_patterns': data.get('year_patterns', True),  # Default to True
                }
            except Exception as e:
                logger.error(f"Error parsing cleaning patterns from database: {e}")
    except Exception as e:
        logger.error(f"Error loading cleaning patterns: {e}")
    finally:
        db.close()
    
    # Return defaults if not found
    return {
        'exact_strings': set(),
        'bracket_patterns': [],
        'parentheses_patterns': [],
        'year_patterns': True,
    }

def clean_movie_name(name, patterns=None):
    """Clean movie name using approved patterns and extract year.
    Can handle both filenames and full paths. For full paths, extracts season/episode info.
    """
    if patterns is None:
        patterns = load_cleaning_patterns()
    
    original_name = name
    year = None
    season = None
    episode = None
    
    # Strip "Path: " prefix if present (from UI display format)
    if name.startswith("Path: "):
        name = name[6:].strip()
    
    # Check if input is a full path (contains path separators)
    is_full_path = '/' in name or '\\' in name
    path_obj = None
    parent_folder = None
    
    if is_full_path:
        # Extract path components
        path_obj = Path(name)
        parent_folder = path_obj.parent.name if path_obj.parent.name else None
        # Use filename for initial cleaning
        name = path_obj.stem
    else:
        # Just a filename, use as-is
        name = name
    
    # STEP 1: Normalize separators and trivial punctuation
    # - Convert runs of '.' or '_' into single spaces
    # - Collapse multiple spaces
    # - Trim leading/trailing spaces/dots/dashes/underscores
    name = re.sub(r'[._]+', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip(' \t.-_')

    # STEP 2: Remove consecutive dots (in case any remain due to other chars)
    name = re.sub(r'\.{2,}', ' ', name)

    # STEP 3: Strip explicit trailing '-' or '.' (defensive after earlier trims)
    name = re.sub(r'[\s\-\.]+$', '', name)

    # Helper: forbidden markers to strip or detect inside brackets
    forbidden_markers = [
        r'rarbg', r'h264', r'vppv', r'yts', r'evo', r'etrg', r'fgp', r'ano',
        r'proper', r'repack', r'rerip', r'sample',
        r'webrip', r'web[-\s]*dl', r'webdl', r'hdtv', r'bluray', r'blu[-\s]*ray',
        r'bdrip', r'brrip', r'remux', r'dvdrip', r'cam', r'ts', r'tc',
        r'x264', r'x265', r'hevc', r'h\.?264', r'h\.?265', r'avc',
        r'aac', r'ac3', r'dts(?:-?hd)?', r'truehd', r'atmos', r'mp3', r'eac3',
        r'2160p', r'1080p', r'720p', r'480p', r'4k', r'uhd',
        r'hdr10?', r'dolby\s*vision', r'\b5\.1\b', r'\b7\.1\b'
    ]
    forbidden_union = r'(?:' + '|'.join(forbidden_markers) + r')'

    # Extract year first if enabled
    if patterns.get('year_patterns', True):
        year = extract_year_from_name(name)
        # If year found, remove everything from the year onwards (including parentheses/brackets around year)
        if year:
            # Pattern to match: optional opening bracket/paren, whitespace, year (with word boundaries), whitespace, optional closing bracket/paren, and everything after
            # This handles: (1971), [1971], {1971}, <1971>, or just 1971
            year_with_context_pattern = rf'(?:[([{{<]\s*)?\b{year}\b\s*(?:[)\]}}>])?.*$'
            # Replace the year and everything after it with empty string
            name = re.sub(year_with_context_pattern, '', name, count=1).strip()
    
    # STEP 4: Remove exact strings (from DB-configured patterns)
    for exact_str in patterns.get('exact_strings', set()):
        name = name.replace(exact_str, ' ')
    
    # STEP 5: Remove bracket content if configured via patterns list
    for pattern in patterns.get('bracket_patterns', []):
        if pattern == '[anything]':
            name = re.sub(r'\[.*?\]', '', name)
        else:
            name = name.replace(pattern, ' ')
    
    # STEP 6: Remove parentheses content if configured via patterns list
    for pattern in patterns.get('parentheses_patterns', []):
        if pattern == '(anything)':
            # Remove parentheses content, but be smart about it
            # Don't remove if it's just a year or looks like part of title
            name = re.sub(r'\([^)]*\)', '', name)
        else:
            name = name.replace(pattern, ' ')

    # STEP 7: Remove common quality/resolution/source/codec/audio tags
    quality_source_patterns = [
        r'\b(?:2160p|1080p|720p|480p|4k|uhd)\b',
        r'\b(?:hdr|hdr10|dolby\s*vision|dv)\b',
        r'\b(?:webrip|web[-\s]*dl|webdl|hdtv|bluray|blu[-\s]*ray|b[dr]rip|remux|dvdrip|cam|ts|tc)\b',
        r'\b(?:x264|x265|hevc|h\.?264|h\.?265|avc)\b',
        r'\b(?:aac|ac3|dts(?:-?hd)?|truehd|atmos|mp3|eac3)\b',
        r'\b(?:5\.1|7\.1)\b',
        r'\b(?:rarbg|vppv|yts|evo|etrg|fgp|ano)\b',
        r'\b(?:h264)\b',
    ]
    for p in quality_source_patterns:
        name = re.sub(p, ' ', name, flags=re.IGNORECASE)

    # STEP 8: Remove edition/packaging flags
    edition_patterns = [
        r'\b(?:proper|repack|rerip)\b',
        r'\b(?:extended|unrated|remastered|final\s*cut|ultimate\s*edition|special\s*edition|theatrical\s*cut)\b',
        r'\b(?:criterion\s*collection)\b',
    ]
    for p in edition_patterns:
        name = re.sub(p, ' ', name, flags=re.IGNORECASE)

    # STEP 9: Extract season/episode info BEFORE removing tags (for TV series)
    episode_title = None
    if is_full_path and path_obj:
        # Extract season from parent folder name (e.g., "Season 1", "Season 01", "S1", "S01")
        parent_str = str(path_obj.parent.name) if path_obj.parent.name else str(path_obj.parent)
        season_match = re.search(r'(?:Season|season)\s*(\d+)', parent_str, re.IGNORECASE)
        if not season_match:
            season_match = re.search(r'\bS(\d+)\b', parent_str, re.IGNORECASE)
        if season_match:
            season = int(season_match.group(1))
        
        # Extract episode from original filename (before cleaning)
        original_filename = path_obj.stem
        episode_match = re.search(r'[_-](\d+)(?:\.|$)', original_filename)
        if not episode_match:
            episode_match = re.search(r'\bE(\d+)\b', original_filename, re.IGNORECASE)
        if not episode_match:
            episode_match = re.search(r'\b(?:ep|episode)\s*(\d+)\b', original_filename, re.IGNORECASE)
        if episode_match:
            episode = int(episode_match.group(1))
        
        # Extract episode title from filename (text after SXXEXX or episode number)
        # For example: "024 S02E01 Points of Departure" -> "024 Points of Departure"
        if season is not None or episode is not None:
            # Try to find text after SXXEXX pattern
            sxxexx_match = re.search(r'\bS\d{2}E\d{2}\b\s*(.+)$', original_filename, re.IGNORECASE)
            if sxxexx_match:
                episode_title = sxxexx_match.group(1).strip()
            else:
                # Try to find text after episode number
                ep_match = re.search(r'\bE\d+\b\s*(.+)$', original_filename, re.IGNORECASE)
                if ep_match:
                    episode_title = ep_match.group(1).strip()
                else:
                    # Try to find text after standalone episode number
                    num_match = re.search(r'^\s*(\d+)\s+(.+)$', original_filename)
                    if num_match:
                        episode_title = original_filename.strip()
        
        # Extract year from parent folder if not found in filename yet
        if year is None and parent_str:
            parent_year = extract_year_from_name(parent_str)
            if parent_year:
                year = parent_year
        
        # If we found season/episode, try to get show name from parent or grandparent folder
        if season is not None or episode is not None:
            # First try parent folder (for cases like "Babylon 5 (1993)")
            parent_name = parent_str
            if parent_name and parent_name.lower() not in ['movies', 'tv', 'series', 'shows', 'video', 'videos', 'season 1', 'season 2', 's1', 's2']:
                # Check if parent folder looks like a show name (not a season folder)
                if not re.search(r'(?:Season|season)\s*\d+|^\s*S\d+\s*$', parent_name, re.IGNORECASE):
                    # Use parent folder as show name, but clean it first
                    show_name = parent_name
                    # Remove year in parentheses
                    show_name = re.sub(r'\([^)]*\)', '', show_name)
                    # Remove common folder patterns
                    show_name = re.sub(r'\[.*?\]', '', show_name)
                    show_name = re.sub(r'\s+', ' ', show_name).strip()
                    if show_name:
                        name = show_name
                        # If we used parent folder, we already extracted year from it above
            else:
                # Fall back to grandparent folder (the show name, skipping the season folder)
                grandparent = path_obj.parent.parent.name if path_obj.parent.parent.name else None
                if grandparent and grandparent.lower() not in ['movies', 'tv', 'series', 'shows', 'video', 'videos']:
                    # Use grandparent folder as show name, but clean it first
                    show_name = grandparent
                    # Remove common folder patterns
                    show_name = re.sub(r'\[.*?\]', '', show_name)
                    show_name = re.sub(r'\(.*?\)', '', show_name)
                    show_name = re.sub(r'\s+', ' ', show_name).strip()
                    if show_name:
                        name = show_name
    
    # Remove season/episode tags from name (they're already extracted)
    # But preserve episode titles that come after SXXEXX (e.g., "S02E01 Points of Departure")
    # Only remove the SXXEXX pattern itself, not text after it
    name = re.sub(r'\bS\d{2}E\d{2}\b\s*', ' ', name, flags=re.IGNORECASE)
    name = re.sub(r'\bSeason\s*\d+\b', ' ', name, flags=re.IGNORECASE)
    # Only remove standalone E\d+ that's not part of SXXEXX (already removed above)
    # Don't remove E\d+ if it's followed by text (episode title)
    name = re.sub(r'\bE\d+\b(?=\s|$)', ' ', name, flags=re.IGNORECASE)
    name = re.sub(r'\b(?:ep|episode)\s*\d+\b', ' ', name, flags=re.IGNORECASE)
    # Remove leading episode numbers (e.g., "024 S02E01" -> remove "024")
    name = re.sub(r'^\s*\d+\s+', ' ', name)
    # Remove episode numbers that are standalone or after dashes/underscores at the start
    name = re.sub(r'^[_-]\d+(?:\.|$)', ' ', name)

    # STEP 10: Remove release group suffixes like "-RARBG", "-YTS", "-EVO" at end
    # Only match if preceded by dash (not space) to avoid removing legitimate title words
    name = re.sub(r'-\b[A-Za-z0-9]{2,10}\b\s*$', ' ', name)

    # STEP 11: Remove language tags when dashed or standalone (e.g., "- FRENCH")
    name = re.sub(r'[\s\-\_]*\b(eng|english|french|german|spanish|italian|russian|japanese|korean|hindi)\b', ' ', name, flags=re.IGNORECASE)

    # STEP 12: Bracket-aware truncation if illegal content found AFTER a leading plain title
    # Pattern: <plain text> <[bracket with forbidden]> <anything>  → keep only <plain text>
    # But DO NOT apply if the name starts with brackets (to avoid losing true title).
    # Supports (), [], {}, <> as brackets
    bracket_any = r'(?:\([^)]*\)|\[[^\]]*\]|\{[^}]*\}|<[^>]*>)'
    m = re.match(r'^(?P<prefix>[^()\[\]{}<>]+?)\s*(?P<bracket>' + bracket_any + r')\s*(?P<suffix>.+)$', name)
    if m:
        prefix = m.group('prefix').strip()
        bracket = m.group('bracket')
        # Extract inner text of the bracket
        inner = re.sub(r'^[\(\[\{<]|[\)\]\}>]$', '', bracket)
        if re.search(forbidden_union, inner, flags=re.IGNORECASE):
            # Only keep prefix; drop bracket and everything after
            name = prefix

    # STEP 13: Normalize leftover punctuation/spaces: remove stray dashes/underscores and extra spaces
    # Preserve dashes that are part of the title (e.g., "L'ultima onda - The Last Wave")
    # Only convert dashes to spaces if they're clearly separators (multiple dashes, or at start/end)
    # Single dashes surrounded by spaces are likely part of the title, so preserve them
    name = re.sub(r'[–—\-]{2,}', ' ', name)  # multiple dashes (separators)
    name = re.sub(r'[–—\-]+\s*$', ' ', name)  # trailing dashes
    name = re.sub(r'^\s*[–—\-]+', ' ', name)  # leading dashes
    name = re.sub(r'\s+', ' ', name).strip(' _-.')

    # STEP 14: Final cleanup for trailing punctuation
    name = re.sub(r'[.\-]+$', '', name).strip()
    
    # Clean up multiple spaces and trim
    name = re.sub(r'\s+', ' ', name).strip()
    
    # If name becomes empty, use original
    if not name:
        name = original_name
    
    # Format TV series name with season/episode if found
    if season is not None or episode is not None:
        season_str = f"S{season:02d}" if season is not None else ""
        episode_str = f"E{episode:02d}" if episode is not None else ""
        if season_str and episode_str:
            name = f"{name} {season_str}{episode_str}"
        elif season_str:
            name = f"{name} {season_str}"
        elif episode_str:
            name = f"{name} {episode_str}"
    
    return name, year

def get_video_length(file_path):
    """Extract video length using video_processing module"""
    return get_video_length_vp(file_path)

def extract_screenshots(video_path, num_screenshots=5, async_mode=True, scan_progress_dict=None):
    """
    Extract screenshots from video using ffmpeg.
    
    These are "screenshots" - frames extracted from the video file itself.
    These are generated by us during scanning, not pre-existing files.
    
    Returns a list of screenshot file paths (existing ones immediately, rest queued for background processing).
    """
    return extract_screenshots_core(video_path, num_screenshots, load_config, find_ffmpeg_core, add_scan_log, async_mode, scan_progress_dict)

def extract_movie_screenshot(video_path, timestamp_seconds=150, async_mode=True):
    """Extract a single screenshot from video - can be synchronous or queued for async processing"""
    if async_mode:
        # Use video_processing module's extract_movie_screenshot which handles async queuing
        return extract_movie_screenshot_core(
            video_path, timestamp_seconds, async_mode,
            load_config, find_ffmpeg_core, scan_progress, add_scan_log
        )
    else:
        # Synchronous mode - import here to avoid circular dependency
        from video_processing import extract_movie_screenshot_sync
        return extract_movie_screenshot_sync(video_path, timestamp_seconds, lambda: find_ffmpeg_core(load_config))

def process_frame_queue(max_workers=3):
    """Process queued frame extractions in background thread pool"""
    process_frame_queue_core(max_workers, scan_progress, add_scan_log)

def index_movie(file_path, db: Session = None):
    """Index a single movie file"""
    # Normalize the path to ensure consistent storage
    # file_path can be either a Path object or a string
    if isinstance(file_path, Path):
        path_obj = file_path
    else:
        path_obj = Path(file_path)
    
    # Use resolve() to get absolute normalized path
    try:
        normalized_path_obj = path_obj.resolve()
    except (OSError, RuntimeError):
        # If resolve fails, use absolute()
        normalized_path_obj = path_obj.absolute()
    
    # Convert to string - Path objects on Windows already use backslashes
    normalized_path = str(normalized_path_obj)
    
    file_hash = get_file_hash(normalized_path)
    
    # Use provided session or create new one
    should_close = False
    if db is None:
        db = SessionLocal()
        should_close = True
    
    try:
        # Check if already indexed and unchanged
        existing = db.query(Movie).filter(Movie.path == normalized_path).first()
        file_unchanged = existing and existing.hash == file_hash
        
        # Check if screenshot exists for this movie
        existing_screenshot = None
        if existing:
            existing_screenshot = db.query(Screenshot).filter(Screenshot.movie_id == existing.id).first()
        has_screenshot = existing_screenshot and os.path.exists(existing_screenshot.shot_path) if existing_screenshot else False
        
        # If file unchanged and screenshot exists, no update needed
        if file_unchanged and has_screenshot:
            return False  # No update needed
        
        add_scan_log("info", f"  Getting file metadata...")
        
        stat = os.stat(normalized_path)
        created = datetime.fromtimestamp(stat.st_ctime)
        size = stat.st_size
        
        # Try to get video length
        length = get_video_length(normalized_path)

        # Exclude files shorter than 60 seconds when length is known
        if length is not None and length < 60:
            add_scan_log("warning", f"  Skipping (too short: {length:.1f}s)")
            # If it exists in DB already, remove it to enforce exclusion
            if existing:
                try:
                    # Delete related screenshots and images first
                    db.query(Screenshot).filter(Screenshot.movie_id == existing.id).delete()
                    db.query(Image).filter(Image.movie_id == existing.id).delete()
                    db.delete(existing)
                    db.commit()
                    add_scan_log("info", f"  Removed existing DB entry for short file")
                except Exception:
                    db.rollback()
            return False
        
        # Find images in folder
        # "images" = media files that came with the movie (posters, covers, etc.)
        add_scan_log("info", f"  Searching for images in folder...")
        images = find_images_in_folder(normalized_path)
        if images:
            add_scan_log("success", f"  Found {len(images)} image(s)")
        
        # Check for existing screenshots in database
        existing_screenshots_list = []
        if existing:
            existing_screenshots_list = [s.shot_path for s in db.query(Screenshot).filter(Screenshot.movie_id == existing.id).all()]
            # Filter to only those that still exist on disk
            existing_screenshots_list = [s for s in existing_screenshots_list if os.path.exists(s)]
        
        # We only take one screenshot per movie now. Use any existing one if present.
        screenshots = existing_screenshots_list
        if len(screenshots) > 0:
            add_scan_log("info", f"  Using existing screenshot")
        
        # Extract one movie screenshot (~3 minutes = 180 seconds)
        add_scan_log("info", f"  Checking screenshot...")
        if existing_screenshot:
            # Check if the screenshot file still exists
            if os.path.exists(existing_screenshot.shot_path):
                add_scan_log("info", f"  Screenshot already exists")
            else:
                # Screenshot file was deleted, remove from DB and queue for re-extraction
                add_scan_log("warning", f"  Screenshot file missing, queuing re-extraction...")
                db.delete(existing_screenshot)
                extract_movie_screenshot(normalized_path, timestamp_seconds=180, async_mode=True)
        else:
            # No screenshot exists, queue for extraction (even if file unchanged)
            add_scan_log("info", f"  No screenshot found, queuing extraction...")
            extract_movie_screenshot(normalized_path, timestamp_seconds=180, async_mode=True)
        
        # Filter out YTS images before storing in database (defense in depth)
        images = filter_yts_images(images)
        
        # Clean movie name and extract year (pass full path to handle TV series with season/episode)
        cleaned_name, year = clean_movie_name(normalized_path)
        
        # Create or update movie record
        if existing:
            # Update existing movie
            existing.name = cleaned_name
            existing.year = year
            existing.length = length
            existing.size = size
            existing.hash = file_hash
            existing.updated = datetime.now()
            movie = existing
        else:
            # Create new movie
            movie = Movie(
                path=normalized_path,
                name=cleaned_name,
                year=year,
                length=length,
                created=created,
                size=size,
                hash=file_hash
            )
            db.add(movie)
            db.flush()  # Flush to get movie.id
        
        # Store images in Image table
        if images:
            # Delete existing images for this movie
            db.query(Image).filter(Image.movie_id == movie.id).delete()
            # Add new images
            for img_path in images:
                image = Image(movie_id=movie.id, image_path=img_path)
                db.add(image)
        
        # Store existing screenshot in DB (background worker adds one if queued above)
        if screenshots:
            # Keep only one path
            shot_path = screenshots[0]
            existing_shot_paths = {s.shot_path for s in db.query(Screenshot).filter(Screenshot.movie_id == movie.id).all()}
            if shot_path not in existing_shot_paths:
                screenshot = Screenshot(movie_id=movie.id, shot_path=shot_path)
                db.add(screenshot)
        
        db.commit()
        return True
    finally:
        if should_close:
            db.close()

def scan_directory(root_path, state=None, progress_callback=None):
    """Scan directory for video files with optional progress callback
    Each movie commits individually - no transaction wrapping the scan.
    If any movie fails, the entire scan stops immediately.
    """
    root = Path(root_path)
    if not root.exists():
        add_scan_log("error", f"Path does not exist: {root_path}")
        raise ValueError(f"Path does not exist: {root_path}")
    
    add_scan_log("info", f"Starting scan of: {root_path}")
    db = SessionLocal()
    try:
        # First pass: count total files
        global scan_progress
        scan_progress["status"] = "counting"
        scan_progress["current_file"] = "Counting files..."
        add_scan_log("info", "Counting video files...")
        
        total_files = 0
        for ext in VIDEO_EXTENSIONS:
            files = [f for f in root.rglob(f"*{ext}") if not is_sample_file(f)]
            count = len(files)
            total_files += count
            if count > 0:
                add_scan_log("info", f"Found {count} {ext} files")
        
        scan_progress["total"] = total_files
        scan_progress["current"] = 0
        scan_progress["status"] = "scanning"
        add_scan_log("success", f"Total files to process: {total_files}")
        
        indexed = 0
        updated = 0
        
        # Second pass: actually scan
        # Each movie commits individually - no transaction wrapping the scan
        # If any movie fails, the entire scan stops immediately
        add_scan_log("info", "Starting file processing...")
        for ext in VIDEO_EXTENSIONS:
            if shutdown_flag.is_set():
                add_scan_log("warning", "Scan interrupted by shutdown")
                break
            for file_path in root.rglob(f"*{ext}"):
                if shutdown_flag.is_set():
                    add_scan_log("warning", "Scan interrupted by shutdown")
                    break
                
                # Skip sample files
                if is_sample_file(file_path):
                    add_scan_log("info", f"Skipping sample file: {file_path.name}")
                    continue
                
                # Process each movie - if it fails, stop the entire scan immediately
                # Each movie commits individually, no transaction wrapping the whole scan
                scan_progress["current"] = indexed + 1
                scan_progress["current_file"] = file_path.name
                
                add_scan_log("info", f"[{indexed + 1}/{total_files}] Processing: {file_path.name}")
                
                if index_movie(file_path, db):
                    updated += 1
                    add_scan_log("success", f"Indexed: {file_path.name}")
                else:
                    add_scan_log("info", f"Skipped (unchanged): {file_path.name}")
                indexed += 1
                
                if progress_callback:
                    progress_callback(indexed, total_files, file_path.name)
        
        # Mark path as indexed
        stmt = sqlite_insert(IndexedPath).values(path=str(root_path))
        # Upsert on UNIQUE(path): update the 'updated' timestamp if it exists
        stmt = stmt.on_conflict_do_update(
            index_elements=[IndexedPath.path],
            set_={"updated": func.now()}
        )
        db.execute(stmt)
        db.commit()
        
        add_scan_log("success", f"Scan complete: {indexed} files processed, {updated} updated")
        
        return {"indexed": indexed, "updated": updated}
    finally:
        db.close()

def run_scan_async(root_path: str):
    """Run scan in background thread"""
    global scan_progress, frame_extraction_queue
    try:
        if shutdown_flag.is_set():
            return
        scan_progress["is_scanning"] = True
        scan_progress["current"] = 0
        scan_progress["total"] = 0
        scan_progress["current_file"] = ""
        scan_progress["status"] = "starting"
        scan_progress["logs"] = []  # Clear previous logs
        scan_progress["frames_processed"] = 0
        scan_progress["frames_total"] = 0
        
        # Clear frame queue
        while not frame_extraction_queue.empty():
            try:
                frame_extraction_queue.get_nowait()
            except:
                break
        
        add_scan_log("info", "=" * 60)
        add_scan_log("info", "Starting movie scan")
        add_scan_log("info", f"Root path: {root_path}")
        add_scan_log("info", "=" * 60)
        
        # Start frame extraction processing in parallel (if not already running)
        process_frame_queue(max_workers=3)
        
        result = scan_directory(root_path, progress_callback=None)
        
        add_scan_log("info", "=" * 60)
        add_scan_log("success", f"Scan completed successfully!")
        add_scan_log("info", f"  Files processed: {result['indexed']}")
        add_scan_log("info", f"  Files updated: {result['updated']}")
        queue_size = frame_extraction_queue.qsize()
        if queue_size > 0:
            add_scan_log("info", f"  Frames queued: {queue_size} (processing in background)")
        add_scan_log("info", "=" * 60)
        
        scan_progress["status"] = "complete"
        scan_progress["is_scanning"] = False
        logger.info(f"Scan complete: {result}")
    except Exception as e:
        # If any movie fails, stop the entire scan immediately
        # Log the error and mark scan as failed - don't continue processing
        error_msg = str(e)
        add_scan_log("error", f"Scan failed: {error_msg}")
        scan_progress["status"] = "error"
        scan_progress["is_scanning"] = False
        logger.error(f"Scan failed: {e}", exc_info=True)
        # Don't re-raise in background thread - just stop and report error

